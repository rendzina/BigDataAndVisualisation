{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Useful links\n",
        "## Data source\n",
        "*Bristol City Council Land and Building Assets*\n",
        "\n",
        "https://www.data.gov.uk/dataset/a98345ad-7f4c-4f6e-882b-e631dc1cc046/bristol-city-council-land-and-building-assets\n",
        "\n",
        "*Bristol dataset*\n",
        "\n",
        "https://www.bristol.gov.uk/files/documents/7241-land-property-2023/file\n",
        "\n",
        "## Useful programming references\n",
        "*Various Spark examples*\n",
        "\n",
        "https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/\n",
        "\n",
        "https://sparkbyexamples.com/pyspark/pyspark-sql-date-and-timestamp-functions/\n",
        "\n",
        "https://spark.apache.org/examples.html\n",
        "\n",
        "https://github.com/apache/spark/tree/master/examples/src/main/python\n",
        "\n",
        "*OReilly's Learning Spark reference*\n",
        "\n",
        "https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch04.html\n",
        "\n",
        "*Recipe Objective - Explain StructType and StructField in PySpark*\n",
        "\n",
        "https://www.projectpro.io/recipes/explain-structtype-and-structfield-pyspark-databricks#:~:text=The%20StructField%20in%20PySpark%20represents,the%20name%20of%20the%20StructField\n",
        "\n",
        "*SPARK StructTypes*\n",
        "\n",
        "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StructType.html#:~:text=.StructType%5Bsource%5D-,Construct%20a%20StructType%20by%20adding%20new%20elements%20to%20it%2C%20to,)%2C%20metadata(optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.pyspark\n",
        "# Load CSV - can be from a standalone local file, or online file source - here is is a local file in storage blob\n",
        "import csv\n",
        "import io\n",
        "from io import StringIO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.pyspark\n",
        "\n",
        "# Note the following will work OK, loading in the CSV file with an inferred schema to the df data frame. But instead, let us demonstrate how to add a defined schema when imorting cSV data to retain more control.\n",
        "#df = spark.read.csv(\"/HdiSamples/BristolCityCouncilLandAndBuildingAssets-2024.csv\", header=True, inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType \n",
        "from pyspark.sql.types import DoubleType, BooleanType, DateType, LongType, FloatType\n",
        "\n",
        "# Using custom schema\n",
        "schema =  StructType() \\\n",
        "      .add(\"Organisation Name\", StringType(), True) \\\n",
        "      .add(\"Organisation Code\", StringType(), True) \\\n",
        "      .add(\"Effective Date\", DateType(), True) \\\n",
        "      .add(\"UPRN\", StringType(), True) \\\n",
        "      .add(\"Property ID\", IntegerType(), True) \\\n",
        "      .add(\"Property Type\", StringType(), True) \\\n",
        "      .add(\"Property Name/Address (Where no UPRN)\", StringType(), True) \\\n",
        "      .add(\"Property Address Detail\", StringType(), True) \\\n",
        "      .add(\"Secondary Address Detail\", StringType(), True) \\\n",
        "      .add(\"Street Number\", StringType(), True) \\\n",
        "      .add(\"Street\", StringType(), True) \\\n",
        "      .add(\"Town / Post Town\", StringType(), True) \\\n",
        "      .add(\"Post Code\", StringType(), True) \\\n",
        "      .add(\"Ward\", StringType(), True) \\\n",
        "      .add(\"Geo X (Easting)\", LongType(), True) \\\n",
        "      .add(\"Geo Y (Northing)\", LongType(), True) \\\n",
        "      .add(\"Tenure Type\", StringType(), True) \\\n",
        "      .add(\"Ground Lease In\", StringType(), True) \\\n",
        "      .add(\"Ground Lease Out\", StringType(), True) \\\n",
        "      .add(\"Lease In to Council\", StringType(), True) \\\n",
        "      .add(\"Lease Out\", StringType(), True) \\\n",
        "      .add(\"Licence In to Council\", StringType(), True) \\\n",
        "      .add(\"Licence Out\", StringType(), True) \\\n",
        "      .add(\"Sub-lease In to Council\", StringType(), True) \\\n",
        "      .add(\"Sub-lease Out\", StringType(), True) \\\n",
        "      .add(\"Vacant\", StringType(), True) \\\n",
        "      .add(\"Asset Type\", StringType(), True) \\\n",
        "      .add(\"Building Size - GIA (M2)\", FloatType(), True) \\\n",
        "      .add(\"Site Area (Hectares)\", FloatType(), True) \\\n",
        "      .add(\"Occupied by Council / Direct Service Property\", StringType(), True) \\\n",
        "      .add(\"Purpose / Asset Category\", StringType(), True)\n",
        "      \n",
        "df = spark.read.format(\"csv\") \\\n",
        "      .options(header=\"True\", inferSchema=\"False\", delimiter=\",\", dateFormat=\"d/M/yyyy\") \\\n",
        "      .schema(schema) \\\n",
        "      .load(\"/HdiSamples/BristolCityCouncilLandAndBuildingAssets-2024.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.pyspark\n",
        "from IPython.display import display\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.pyspark\n",
        "# Show schema\n",
        "spark.sql(\"DESCRIBE TABLE ss01shh_Bristol\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.pyspark\n",
        "# Show dataframe\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.pyspark\n",
        "# The data is now ready to use. First let's take the df and create a new virtial HIVE table to query in Spark with SPARK-SQL\n",
        "\n",
        "# Register the dataframe as a virtual HIVE table to allow SparkSQL\n",
        "#df.registerTempTable('BristolCouncilAssets') # depracated form of command, nopw replaced with ...\n",
        "df.createOrReplaceTempView(\"ss01shh_Bristol\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " # Problem 1\n",
        "How many ‘Properties’ are Bristol City Council (BCC) responsible for (as owner, user or manager)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.pyspark\n",
        "#Use PySpark commands to query dataframe\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.pyspark\n",
        "# Alternatively, use SPARK-SQL to query the HIVE table we created\n",
        "spark.sql(\"SELECT count(*) from ss01shh_Bristol\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " # Problem 2\n",
        "\n",
        "How many unique ‘Property Type’s are Bristol City Council (BCC) responsible for (as owner, user or manager)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.pyspark\n",
        "spark.sql(\"SELECT DISTINCT `Property Type` FROM ss01shh_Bristol ORDER BY `Property Type`\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " The above output is broken down to show all the seperate property types - if we just want a total count then we can do as is shown below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.pyspark\n",
        "spark.sql(\"SELECT Count(*) AS Count FROM (SELECT DISTINCT `Property Type` FROM ss01shh_Bristol) types\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem 3\n",
        "\n",
        "How many properties are there in each of the ‘Property Types’?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.pyspark\n",
        "spark.sql(\"SELECT `Property Type`, Count(*) AS Count FROM ss01shh_Bristol GROUP BY `Property Type` ORDER BY `Property Type`\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.sql\n",
        "-- The same data but an alternative view using livy2.sql magic - now we can use the built in graphics (note as this is sql, comments start with '--')\n",
        "SELECT `Property Type`, Count(*) AS Count FROM ss01shh_Bristol GROUP BY `Property Type` ORDER BY Count DESC LIMIT 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem 4\n",
        "\n",
        "Show a histogram classifying the total area (in Ha) of each of these 'property types'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.pyspark\n",
        "area_data = spark.sql(\"SELECT `Property Type`, Sum(`Site Area (Hectares)`) AS Total_Area FROM ss01shh_Bristol GROUP BY `Property Type` ORDER BY Sum(`Site Area (Hectares)`) DESC LIMIT 15\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.pyspark\n",
        "# Create bar chart for total area by property type\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Use non-interactive backend for Zeppelin\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import io\n",
        "import base64\n",
        "\n",
        "# Initialize Zeppelin display object\n",
        "try:\n",
        "    from zeppelin import Zeppelin\n",
        "    z = Zeppelin()\n",
        "    print(\"Zeppelin object initialized successfully\")\n",
        "except ImportError:\n",
        "    try:\n",
        "        # Alternative import method\n",
        "        import zeppelin\n",
        "        z = zeppelin\n",
        "        print(\"Zeppelin imported as module\")\n",
        "    except ImportError:\n",
        "        print(\"Zeppelin module not available - will use alternative methods\")\n",
        "\n",
        "#plt.close('all') \n",
        "\n",
        "# For the data source, we will use the data query above - which created 'area_data' from the SQL query\n",
        "area_data = spark.sql(\"SELECT `Property Type`, Sum(`Site Area (Hectares)`) AS Total_Area FROM ss01shh_Bristol GROUP BY `Property Type` ORDER BY Sum(`Site Area (Hectares)`) DESC LIMIT 15\")\n",
        "\n",
        "# Convert to Pandas DataFrame for plotting\n",
        "area_df = area_data.toPandas()\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(14, 8))\n",
        "bars = plt.bar(range(len(area_df)), area_df['Total_Area'], color='steelblue', alpha=0.7)\n",
        "\n",
        "# Customise the chart\n",
        "plt.title('Total Site Area by Property Type (Top 15)', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Property Type', fontsize=12)\n",
        "plt.ylabel('Total Area (Hectares)', fontsize=12)\n",
        "plt.xticks(range(len(area_df)), area_df['Property Type'], rotation=45, ha='right')\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for i, bar in enumerate(bars):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
        "             f'{height:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Adjust layout to prevent label cutoff\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot output\n",
        "show(plt)\n",
        "\n",
        "\n",
        "# Display the data table as well\n",
        "#print(\"\\nData used for the chart:\")\n",
        "#area_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.sql\n",
        "-- ALTERNATIVE METHOD: Use Zeppelin's built-in SQL visualization\n",
        "-- This is the most reliable method for charts in Zeppelin\n",
        "-- The chart will appear automatically when you run this cell\n",
        "SELECT \n",
        "    `Property Type` as property_type,\n",
        "    ROUND(SUM(`Site Area (Hectares)`), 2) as total_area_hectares\n",
        "FROM ss01shh_Bristol \n",
        "GROUP BY `Property Type` \n",
        "ORDER BY total_area_hectares DESC\n",
        "LIMIT 15\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem 5\n",
        "\n",
        "For each of these grouped properties, what are the numbers of properties in each ‘tenure type’ recorded?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.pyspark\n",
        "spark.sql(\"SELECT `Property Type`, `Tenure Type`, COUNT(*) AS Count FROM ss01shh_Bristol GROUP BY `Tenure Type`, `Property Type` ORDER BY `Property Type` DESC\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%livy2.pyspark\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "zeppelin",
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "BristolCityCouncilPropertyExample_Zeppelin"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
