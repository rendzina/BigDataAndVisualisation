{
  "metadata": {
    "name": "BristolCityCouncilPropertyExample_Zeppelin",
    "kernelspec": {
 "display_name":"zeppelin",     "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Bristol City Council Land and Building Assets challenge\n\n## Unit 2 Problem Definition and Suggested Solutions - Zeppelin Version\n### Useful links\n\n#### Data source\n\n[https://www.data.gov.uk/dataset/a98345ad-7f4c-4f6e-882b-e631dc1cc046/bristol-city-council-land-and-building-assets](https://www.data.gov.uk/dataset/a98345ad-7f4c-4f6e-882b-e631dc1cc046/bristol-city-council-land-and-building-assets)\n\n[https://www.bristol.gov.uk/files/documents/7241-land-property-2023/file]([https://www.bristol.gov.uk/files/documents/7241-land-property-2023/file])\n\n#### Programming\n\n[https://www.projectpro.io/recipes/explain-structtype-and-structfield-pyspark-databricks#:~:text\u003dThe%20StructField%20in%20PySpark%20represents,the%20name%20of%20the%20StructField](https://www.projectpro.io/recipes/explain-structtype-and-structfield-pyspark-databricks#:~:text\u003dThe%20StructField%20in%20PySpark%20represents,the%20name%20of%20the%20StructField)\n\n[https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StructType.html#:~:text\u003d.StructType%5Bsource%5D-,Construct%20a%20StructType%20by%20adding%20new%20elements%20to%20it%2C%20to,)%2C%20metadata(optional)](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StructType.html#:~:text\u003d.StructType%5Bsource%5D-,Construct%20a%20StructType%20by%20adding%20new%20elements%20to%20it%2C%20to,)%2C%20metadata(optional))\n\n[https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/](https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/)\n\n[https://sparkbyexamples.com/pyspark/pyspark-sql-date-and-timestamp-functions/](https://sparkbyexamples.com/pyspark/pyspark-sql-date-and-timestamp-functions/)\n\n[https://spark.apache.org/examples.html](https://spark.apache.org/examples.html]) and [https://github.com/apache/spark/tree/master/examples/src/main/python](https://github.com/apache/spark/tree/master/examples/src/main/python)\n\n[https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch04.html](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch04.html)"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.pyspark\n# Load CSV - can be from a standalone local file, or online file source - here is is a local file in storage blob\nimport csv\nimport io\nfrom io import StringIO\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.pyspark\n\n# Note the following will work OK, loading in the CSV file with an inferred schema to the df data frame. But instead, let us demonstrate how to add a defined schema when imorting cSV data to retain more control.\n#df \u003d spark.read.csv(\"/HdiSamples/BristolCityCouncilLandAndBuildingAssets-2024.csv\", header\u003dTrue, inferSchema\u003dTrue)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.pyspark\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType \nfrom pyspark.sql.types import DoubleType, BooleanType, DateType, LongType, FloatType\n\n# Using custom schema\nschema \u003d  StructType() \\\n      .add(\"Organisation Name\", StringType(), True) \\\n      .add(\"Organisation Code\", StringType(), True) \\\n      .add(\"Effective Date\", DateType(), True) \\\n      .add(\"UPRN\", StringType(), True) \\\n      .add(\"Property ID\", IntegerType(), True) \\\n      .add(\"Property Type\", StringType(), True) \\\n      .add(\"Property Name/Address (Where no UPRN)\", StringType(), True) \\\n      .add(\"Property Address Detail\", StringType(), True) \\\n      .add(\"Secondary Address Detail\", StringType(), True) \\\n      .add(\"Street Number\", StringType(), True) \\\n      .add(\"Street\", StringType(), True) \\\n      .add(\"Town / Post Town\", StringType(), True) \\\n      .add(\"Post Code\", StringType(), True) \\\n      .add(\"Ward\", StringType(), True) \\\n      .add(\"Geo X (Easting)\", LongType(), True) \\\n      .add(\"Geo Y (Northing)\", LongType(), True) \\\n      .add(\"Tenure Type\", StringType(), True) \\\n      .add(\"Ground Lease In\", StringType(), True) \\\n      .add(\"Ground Lease Out\", StringType(), True) \\\n      .add(\"Lease In to Council\", StringType(), True) \\\n      .add(\"Lease Out\", StringType(), True) \\\n      .add(\"Licence In to Council\", StringType(), True) \\\n      .add(\"Licence Out\", StringType(), True) \\\n      .add(\"Sub-lease In to Council\", StringType(), True) \\\n      .add(\"Sub-lease Out\", StringType(), True) \\\n      .add(\"Vacant\", StringType(), True) \\\n      .add(\"Asset Type\", StringType(), True) \\\n      .add(\"Building Size - GIA (M2)\", FloatType(), True) \\\n      .add(\"Site Area (Hectares)\", FloatType(), True) \\\n      .add(\"Occupied by Council / Direct Service Property\", StringType(), True) \\\n      .add(\"Purpose / Asset Category\", StringType(), True)\n      \ndf \u003d spark.read.format(\"csv\") \\\n      .options(header\u003d\"True\", inferSchema\u003d\"False\", delimiter\u003d\",\", dateFormat\u003d\"d/M/yyyy\") \\\n      .schema(schema) \\\n      .load(\"/HdiSamples/BristolCityCouncilLandAndBuildingAssets-2024.csv\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.pyspark\nfrom IPython.display import display\ndisplay(df)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.pyspark\n# Show schema\nspark.sql(\"DESCRIBE TABLE ss01shh_Bristol\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.pyspark\n# Show dataframe\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.pyspark\n# The data is now ready to use. First let\u0027s take the df and create a new virtial HIVE table to query in Spark with SPARK-SQL\n\n# Register the dataframe as a virtual HIVE table to allow SparkSQL\n#df.registerTempTable(\u0027BristolCouncilAssets\u0027) # depracated form of command, nopw replaced with ...\ndf.createOrReplaceTempView(\"ss01shh_Bristol\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " # Problem 1\nHow many ‘Properties’ are Bristol City Council (BCC) responsible for (as owner, user or manager)?"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.pyspark\n#Use PySpark commands to query dataframe\ndf.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.pyspark\n# Alternatively, use SPARK-SQL to query the HIVE table we created\nspark.sql(\"SELECT count(*) from ss01shh_Bristol\").show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " # Problem 2\n\nHow many unique ‘Property Type’s are Bristol City Council (BCC) responsible for (as owner, user or manager)?"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.pyspark\nspark.sql(\"SELECT DISTINCT `Property Type` FROM ss01shh_Bristol ORDER BY `Property Type`\").show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " The above output is broken down to show all the seperate property types - if we just want a total count then we can do as is shown below.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.pyspark\nspark.sql(\"SELECT Count(*) AS Count FROM (SELECT DISTINCT `Property Type` FROM ss01shh_Bristol) types\").show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Problem 3\n\nHow many properties are there in each of the ‘Property Types’?"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.pyspark\nspark.sql(\"SELECT `Property Type`, Count(*) AS Count FROM ss01shh_Bristol GROUP BY `Property Type` ORDER BY `Property Type`\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.sql\n-- The same data but an alternative view using livy2.sql magic - now we can use the built in graphics (note as this is sql, comments start with \u0027--\u0027)\nSELECT `Property Type`, Count(*) AS Count FROM ss01shh_Bristol GROUP BY `Property Type` ORDER BY Count DESC LIMIT 15"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Problem 4\n\nShow a histogram classifying the total area (in Ha) of each of these \u0027property types\u0027\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.pyspark\narea_data \u003d spark.sql(\"SELECT `Property Type`, Sum(`Site Area (Hectares)`) AS Total_Area FROM ss01shh_Bristol GROUP BY `Property Type` ORDER BY Sum(`Site Area (Hectares)`) DESC LIMIT 15\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.pyspark\n# Create bar chart for total area by property type\nimport matplotlib\nmatplotlib.use(\u0027Agg\u0027)  # Use non-interactive backend for Zeppelin\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport io\nimport base64\n\n# Initialize Zeppelin display object\ntry:\n    from zeppelin import Zeppelin\n    z \u003d Zeppelin()\n    print(\"Zeppelin object initialized successfully\")\nexcept ImportError:\n    try:\n        # Alternative import method\n        import zeppelin\n        z \u003d zeppelin\n        print(\"Zeppelin imported as module\")\n    except ImportError:\n        print(\"Zeppelin module not available - will use alternative methods\")\n\n#plt.close(\u0027all\u0027) \n\n# For the data source, we will use the data query above - which created \u0027area_data\u0027 from the SQL query\narea_data \u003d spark.sql(\"SELECT `Property Type`, Sum(`Site Area (Hectares)`) AS Total_Area FROM ss01shh_Bristol GROUP BY `Property Type` ORDER BY Sum(`Site Area (Hectares)`) DESC LIMIT 15\")\n\n# Convert to Pandas DataFrame for plotting\narea_df \u003d area_data.toPandas()\n\n# Create the bar chart\nplt.figure(figsize\u003d(14, 8))\nbars \u003d plt.bar(range(len(area_df)), area_df[\u0027Total_Area\u0027], color\u003d\u0027steelblue\u0027, alpha\u003d0.7)\n\n# Customise the chart\nplt.title(\u0027Total Site Area by Property Type (Top 15)\u0027, fontsize\u003d16, fontweight\u003d\u0027bold\u0027)\nplt.xlabel(\u0027Property Type\u0027, fontsize\u003d12)\nplt.ylabel(\u0027Total Area (Hectares)\u0027, fontsize\u003d12)\nplt.xticks(range(len(area_df)), area_df[\u0027Property Type\u0027], rotation\u003d45, ha\u003d\u0027right\u0027)\n\n# Add value labels on top of bars\nfor i, bar in enumerate(bars):\n    height \u003d bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n             f\u0027{height:.1f}\u0027, ha\u003d\u0027center\u0027, va\u003d\u0027bottom\u0027, fontsize\u003d9)\n\n# Adjust layout to prevent label cutoff\nplt.tight_layout()\nplt.grid(axis\u003d\u0027y\u0027, alpha\u003d0.3)\n\n# Plot output\nshow(plt)\n\n\n# Display the data table as well\n#print(\"\\nData used for the chart:\")\n#area_df\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.sql\n-- ALTERNATIVE METHOD: Use Zeppelin\u0027s built-in SQL visualization\n-- This is the most reliable method for charts in Zeppelin\n-- The chart will appear automatically when you run this cell\nSELECT \n    `Property Type` as property_type,\n    ROUND(SUM(`Site Area (Hectares)`), 2) as total_area_hectares\nFROM ss01shh_Bristol \nGROUP BY `Property Type` \nORDER BY total_area_hectares DESC\nLIMIT 15\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Problem 5\n\nFor each of these grouped properties, what are the numbers of properties in each ‘tenure type’ recorded?"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.pyspark\nspark.sql(\"SELECT `Property Type`, `Tenure Type`, COUNT(*) AS Count FROM ss01shh_Bristol GROUP BY `Tenure Type`, `Property Type` ORDER BY `Property Type` DESC\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%livy2.pyspark\n"
    }
  ]
}